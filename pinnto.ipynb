{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb95001e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from numpy.lib.stride_tricks import as_strided\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy.io as sio\n",
    "from scipy.sparse import coo_matrix\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from scipy import ndimage, misc\n",
    "import time\n",
    "import os    \n",
    "import random\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "#%% Check GPU availability\n",
    "print(f\"Pytorch Version: {torch.__version__}\")\n",
    "print(\"GPU is\", \"available\" if torch.cuda.is_available() else \"NOT AVAILABLE\")\n",
    "\n",
    "# print(torch.cuda.get_device_name(torch.cuda.current_device()))   \n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#%% set seed\n",
    "\n",
    "manualSeed = 1234\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed(manualSeed)\n",
    "torch.cuda.manual_seed_all(manualSeed)\n",
    "np.random.seed(manualSeed)\n",
    "random.seed(manualSeed)\n",
    "\n",
    "#%% Pooling operation for density-interpolation scheme\n",
    "def pool2d(A, kernel_size, stride, padding=0, pool_mode='max'):\n",
    "    # Padding\n",
    "   A = np.pad(A, padding, mode='constant')\n",
    "\n",
    "    # Window view of A\n",
    "   output_shape = ((A.shape[0] - kernel_size) // stride + 1,\n",
    "                    (A.shape[1] - kernel_size) // stride + 1)\n",
    "    \n",
    "   shape_w = (output_shape[0], output_shape[1], kernel_size, kernel_size)\n",
    "   strides_w = (stride*A.strides[0], stride*A.strides[1], A.strides[0], A.strides[1])\n",
    "    \n",
    "   A_w = as_strided(A, shape_w, strides_w)\n",
    "\n",
    "   # Return the result of pooling\n",
    "   if pool_mode == 'max':\n",
    "       return A_w.max(axis=(2, 3))\n",
    "   elif pool_mode == 'avg':\n",
    "       return A_w.mean(axis=(2, 3))\n",
    "\n",
    "#%% Optimality Criterion\n",
    "def oc(nelx,nely,x,volfrac,dc,dv,g):\n",
    "\tl1=0\n",
    "\tl2=1e9\n",
    "\tmove=0.2\n",
    "\t# reshape to perform vector operations\n",
    "\txnew=np.zeros(nelx*nely)\n",
    "\twhile (l2-l1)/(l1+l2)>1e-3:\n",
    "\t\tlmid=0.5*(l2+l1)\n",
    "\t\txnew[:]= np.maximum(0.0,np.maximum(x-move,np.minimum(1.0,np.minimum(x+move,x*np.sqrt(-dc/dv/lmid)))))\n",
    "\t\tgt=g+np.sum((dv*(xnew-x)))\n",
    "\t\tif gt>0 :\n",
    "\t\t\tl1=lmid\n",
    "\t\telse:\n",
    "\t\t\tl2=lmid\n",
    "\treturn (xnew,gt)\n",
    "#%% Energy-based PINN neural network\n",
    "class Net(nn.Module):\n",
    "\n",
    "    def __init__(self, n_input, n_output, n_layer, n_nodes):\n",
    "        super(Net, self).__init__()\n",
    "        self.n_layer = n_layer\n",
    "        self.Input = nn.Linear(n_input, n_nodes)   # linear layer\n",
    "        nn.init.xavier_uniform_(self.Input.weight) # wigths and bias initiation\n",
    "        nn.init.normal_(self.Input.bias)\n",
    "        \n",
    "        self.Output = nn.Linear(n_nodes, n_output)\n",
    "        nn.init.xavier_uniform_(self.Output.weight)\n",
    "        nn.init.normal_(self.Output.bias)\n",
    "\n",
    "        self.Hidden = nn.ModuleList() # hidden layer list\n",
    "        for i in range(n_layer):\n",
    "            self.Hidden.append(nn.Linear(n_nodes, n_nodes))\n",
    "        for layer in self.Hidden:\n",
    "            nn.init.xavier_uniform_(layer.weight)\n",
    "            nn.init.normal_(layer.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y = torch.tanh(self.Input(x)) # tanh activation function\n",
    "        for layer in self.Hidden:\n",
    "            y = torch.tanh(layer(y))\n",
    "        y = self.Output(y)\n",
    "        return y\n",
    "    \n",
    "#%% Automatic differentiation\n",
    "def derivative(x, Net, func, order):\n",
    "    \n",
    "    w = Net(x)*func(x).view(-1,1)\n",
    "    \n",
    "    if order == '0':\n",
    "        return w\n",
    "    \n",
    "    else:\n",
    "        dw_xy = torch.autograd.grad(w, x, torch.ones_like(w), \n",
    "                                    retain_graph=True, create_graph=True)\n",
    "        dw_x = dw_xy[0][:,0].view(-1,1)\n",
    "        dw_y = dw_xy[0][:,1].view(-1,1)\n",
    "        \n",
    "        if order == '1':\n",
    "            return w, dw_x, dw_y\n",
    "    \n",
    "        else:\n",
    "            dw_xxy = torch.autograd.grad(dw_x, x, torch.ones_like(dw_x), \n",
    "                                         retain_graph=True, create_graph=True)\n",
    "            dw_xx = dw_xxy[0][:,0].view(-1,1)\n",
    "            dw_xy = dw_xxy[0][:,1].view(-1,1)\n",
    "            dw_yy = torch.autograd.grad(dw_y, x, torch.ones_like(dw_y), retain_graph=True, \n",
    "                                        create_graph=True)[0][:,1].view(-1,1)\n",
    "            return w, dw_x, dw_y, dw_xx, dw_yy, dw_xy\n",
    "\n",
    "#%%\n",
    "# Domain data\n",
    "nelx, nely = 40, 20\n",
    "ndof = 2*(nelx+1)*(nely+1)\n",
    "x, y = np.meshgrid(np.linspace(0,2,nelx + 1), np.linspace(0,1,nely + 1))\n",
    "y = np.flipud(y)\n",
    "data = np.hstack([x.reshape(-1,1, order = 'F'), y.reshape(-1,1, order = 'F')])\n",
    "\n",
    "# for contourf\n",
    "X, Y = np.meshgrid(np.linspace(0,2,nelx), np.linspace(0,1,nely))\n",
    "Y = np.flipud(Y)\n",
    "\n",
    "# find boundary\n",
    "idx_f = np.where((data[:,0]==max(data[:,0])) &  (data[:,1]==0.5))\n",
    "data = torch.tensor(data, dtype=torch.float32, requires_grad=True, device=device)\n",
    "\n",
    "#%%\n",
    "# BC's and support\n",
    "dofs=np.arange(2*(nelx+1)*(nely+1))\n",
    "# fixed=np.union1d(dofs[0:2*(nely+1):2],np.array([2*(nelx+1)*(nely+1)-1]))\n",
    "fixed = np.arange(2*(nely+1))\n",
    "free=np.setdiff1d(dofs,fixed)\n",
    "\n",
    "# Solution and RHS vectors\n",
    "f=np.zeros((ndof,1))\n",
    "u_fea=np.zeros((ndof,1))\n",
    "# Set load\n",
    "f[2*(nelx+1)*(nely+1)-(nely+1),0]=-1\n",
    "#%% codes from 88-line topology optimization\n",
    "#element stiffness matrix\n",
    "def lk():\n",
    "\tE=1\n",
    "\tnu=0.3\n",
    "\tk=np.array([1/2-nu/6,1/8+nu/8,-1/4-nu/12,-1/8+3*nu/8,-1/4+nu/12,-1/8-nu/8,nu/6,1/8-3*nu/8])\n",
    "\tKE = E/(1-nu**2)*np.array([ [k[0], k[1], k[2], k[3], k[4], k[5], k[6], k[7]],\n",
    "\t[k[1], k[0], k[7], k[6], k[5], k[4], k[3], k[2]],\n",
    "\t[k[2], k[7], k[0], k[5], k[6], k[3], k[4], k[1]],\n",
    "\t[k[3], k[6], k[5], k[0], k[7], k[2], k[1], k[4]],\n",
    "\t[k[4], k[5], k[6], k[7], k[0], k[1], k[2], k[3]],\n",
    "\t[k[5], k[4], k[3], k[2], k[1], k[0], k[7], k[6]],\n",
    "\t[k[6], k[3], k[4], k[1], k[2], k[7], k[0], k[5]],\n",
    "\t[k[7], k[2], k[1], k[4], k[3], k[6], k[5], k[0]] ]);\n",
    "\treturn (KE)\n",
    "\n",
    "dv = np.ones(nely*nelx)\n",
    "dc = np.ones(nely*nelx)\n",
    "ce = np.ones(nely*nelx)\n",
    "ce_fea = np.ones(nely*nelx)\n",
    "rmin = 3\n",
    "\n",
    "KE=lk()\n",
    "edofMat=np.zeros((nelx*nely,8),dtype=int)\n",
    "for elx in range(nelx):\n",
    "\tfor ely in range(nely):\n",
    "\t\tel = ely+elx*nely\n",
    "\t\tn1=(nely+1)*elx+ely\n",
    "\t\tn2=(nely+1)*(elx+1)+ely\n",
    "\t\tedofMat[el,:]=np.array([2*n1+2, 2*n1+3, 2*n2+2, 2*n2+3,2*n2, 2*n2+1, 2*n1, 2*n1+1])\n",
    "# Construct the index pointers for the coo format\n",
    "iK = np.kron(edofMat,np.ones((8,1))).flatten()\n",
    "jK = np.kron(edofMat,np.ones((1,8))).flatten()   \n",
    "# Filter: Build (and assemble) the index+data vectors for the coo matrix format\n",
    "nfilter=int(nelx*nely*((2*(np.ceil(rmin)-1)+1)**2))\n",
    "iH = np.zeros(nfilter)\n",
    "jH = np.zeros(nfilter)\n",
    "sH = np.zeros(nfilter)\n",
    "cc=0\n",
    "for i in range(nelx):\n",
    "\tfor j in range(nely):\n",
    "\t\trow=i*nely+j\n",
    "\t\tkk1=int(np.maximum(i-(np.ceil(rmin)-1),0))\n",
    "\t\tkk2=int(np.minimum(i+np.ceil(rmin),nelx))\n",
    "\t\tll1=int(np.maximum(j-(np.ceil(rmin)-1),0))\n",
    "\t\tll2=int(np.minimum(j+np.ceil(rmin),nely))\n",
    "\t\tfor k in range(kk1,kk2):\n",
    "\t\t\tfor l in range(ll1,ll2):\n",
    "\t\t\t\tcol=k*nely+l\n",
    "\t\t\t\tfac=rmin-np.sqrt(((i-k)*(i-k)+(j-l)*(j-l)))\n",
    "\t\t\t\tiH[cc]=row\n",
    "\t\t\t\tjH[cc]=col\n",
    "\t\t\t\tsH[cc]=np.maximum(0.0,fac)\n",
    "\t\t\t\tcc=cc+1\n",
    "# Finalize assembly and convert to csc format\n",
    "H=coo_matrix((sH,(iH,jH)),shape=(nelx*nely,nelx*nely)).tocsc()\t\n",
    "Hs=H.sum(1)\n",
    "#%%\n",
    "\n",
    "# plain strain parameters\n",
    "mu = 0.3\n",
    "P = 1\n",
    "vol = 1\n",
    "volfrac = 0.5\n",
    "Emin=1e-9\n",
    "Emax=10.0\n",
    "penal=3.0\n",
    "g = 0\n",
    "\n",
    "xPhys=volfrac * np.ones(nely*nelx,dtype=float)\n",
    "\n",
    "new_domain = volfrac *torch.ones([(nelx+1)*(nely+1),1])\n",
    "loss_hist = []\n",
    "#%% Initialize model\n",
    "Net_u = Net(2, 1, 8, 80)\n",
    "Net_v = Net(2, 1, 8, 80)\n",
    "\n",
    "func_u = lambda x:  x[:,0]\n",
    "func_v = lambda x:  x[:,1]\n",
    "epochs = 2000\n",
    "learning_rate = 1.0e-3\n",
    "reduce_lr_after = max([10, epochs / 10])\n",
    "#%% Model train\n",
    "for i in range(100):\n",
    "\n",
    "    # Construct neural network\n",
    "    Net_u = Net(2, 1, 8, 80).to(device)\n",
    "    Net_v = Net(2, 1, 8, 80).to(device)\n",
    "\n",
    "    optimizer = torch.optim.AdamW(list(Net_u.parameters())+list(Net_v.parameters()), lr=learning_rate)\n",
    "    reduce_lr_after = max([10, epochs / 10])\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, \n",
    "                                                            'min', \n",
    "                                                            factor = 0.5,\n",
    "                                                            patience = reduce_lr_after, \n",
    "                                                            min_lr=0, \n",
    "                                                            threshold=0.001, \n",
    "                                                            verbose=False)\n",
    "    \n",
    "    time_1 = time.time()\n",
    "    #### Material Interpolation scheme #####\n",
    "    E = Emin + new_domain**penal*(Emax-Emin) \n",
    "    E = E.to(device) # send tensor to GPU (if GPU is available)\n",
    "    \n",
    "    # Start training for energy-based PINN\n",
    "    for epoch in range(epochs):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Calculate derivatives\n",
    "        u = Net_u(data)*func_u(data).view(-1,1)\n",
    "        v = Net_v(data)*func_u(data).view(-1,1)\n",
    "        \n",
    "        du_xy = torch.autograd.grad(u, data, torch.ones_like(u), retain_graph=True, create_graph=True)\n",
    "        dv_xy = torch.autograd.grad(v, data, torch.ones_like(v), retain_graph=True, create_graph=True)\n",
    "        \n",
    "        du_x = du_xy[0][:,0].view(-1,1)\n",
    "        du_y = du_xy[0][:,1].view(-1,1)\n",
    "        dv_x = dv_xy[0][:,0].view(-1,1)\n",
    "        dv_y = dv_xy[0][:,1].view(-1,1)\n",
    "        \n",
    "        # strain\n",
    "        ex = du_x\n",
    "        ey = dv_y\n",
    "        \n",
    "        gxy = dv_x + du_y\n",
    "        \n",
    "        exy = 0.5 * gxy\n",
    "    \n",
    "        # stress\n",
    "        G = E / (2*(1+mu))\n",
    "        G = G.to(device)\n",
    "        e = ex + ey\n",
    "        lam = mu*E / ((1+mu) * (1 - 2*mu))\n",
    "        \n",
    "        sx = 2*G*ex + lam*e\n",
    "        sy = 2*G*ey + lam*e\n",
    "    \n",
    "        txy = G * gxy\n",
    "    \n",
    "        # Internal strain energy\n",
    "        Eint = torch.mean( 1/(2*E)*(sx**2 + sy**2 ) - (mu/E)*(sx*sy) + (1/(2*G)*(txy**2)))\n",
    "        \n",
    "        # External strain energy\n",
    "        Eext = Net_v(data[idx_f])*-P\n",
    "        \n",
    "        # Loss\n",
    "        loss = Eint-Eext\n",
    "            \n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        scheduler.step(loss)\n",
    "    \n",
    "    loss_hist.append(loss.item())\n",
    "    time_2 = time.time()\n",
    "    time_interval = time_2 - time_1\n",
    "    \n",
    "    #%% Predict Displacement x,y\n",
    "    upred = u.cpu().detach().numpy() \n",
    "    vpred = v.cpu().detach().numpy() \n",
    "    \n",
    "    disp = np.ravel([upred,vpred],'F')\n",
    "    #%% Sensitivity Analysis\n",
    "    ce[:] = (np.dot(disp[edofMat].reshape(nelx*nely,8),KE) * disp[edofMat].reshape(nelx*nely,8) ).sum(1)\n",
    "    obj = ((Emin+xPhys**penal*(Emax-Emin))*ce ).sum()\n",
    "    dc[:] = (-penal*xPhys**(penal-1)*(Emax-Emin))*ce\n",
    "    dv[:] = np.ones(nely*nelx)\n",
    "    \n",
    "    #%% FEA for validation purpose\n",
    "    # sK=((KE.flatten()[np.newaxis]).T*(Emin+(xPhys)**penal*(Emax-Emin))).flatten(order='F')\n",
    "    # K = coo_matrix((sK,(iK,jK)),shape=(ndof,ndof)).tocsc()\n",
    "    # K = K[free,:][:,free]\n",
    "    # u_fea[free,0]=spsolve(K,f[free,0])\n",
    "    # ce_fea[:] = (np.dot(u_fea[edofMat].reshape(nelx*nely,8),KE) * u_fea[edofMat].reshape(nelx*nely,8) ).sum(1)\n",
    "    # obj_fea = ((Emin+xPhys**penal*(Emax-Emin))*ce_fea ).sum()\n",
    "    \n",
    "    #%% Sensitivity filter\n",
    "    dc[:] = np.asarray((H*(xPhys*dc))[np.newaxis].T/Hs)[:,0] / np.maximum(0.001,xPhys)\n",
    "    \n",
    "    #%% design update using Optimality Criterion\n",
    "    (xPhys[:],g)=oc(nelx,nely,xPhys,volfrac,dc,dv,g)\n",
    "    \n",
    "    #%% Show new design\n",
    "    new_domain = xPhys.reshape((nelx,nely)).T\n",
    "    volume = np.mean(new_domain)\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(-new_domain, cmap='gray')\n",
    "\n",
    "    ax.axis('equal')\n",
    "    ax.axis('off')\n",
    "    plt.show()\n",
    "    #%% Density Interpolation Scheme\n",
    "    new_domain = pool2d(new_domain, kernel_size=2, stride=1, padding=1, pool_mode='max').reshape([-1,1],order='F')\n",
    "    new_domain = torch.tensor(new_domain)\n",
    "    \n",
    "    #%% Print result\n",
    "    print('[Iteration: %d] [time: %.2f sec] [obj: %.4f] [obj_fea: %.4f] [vol: %.2f]' % (i+1, time_interval, obj, obj_fea, volume))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
